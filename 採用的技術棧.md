# Proxy IP Pool Collector - 技術棧選擇

## 1. 技術棧總覽

### 1.1 核心技術選擇原則

- **高性能**: 支援高併發異步處理
- **穩定性**: 成熟的技術棧，社區活躍
- **可擴展性**: 模組化設計，易於維護和擴展
- **易用性**: 開發效率高，學習成本低

### 1.2 架構分層技術選型

根據《ETL架構規格書.md》、《後端架構規格書.md》、《前端架構規格書.md》，技術選型如下：

```
┌─────────────────────────────────────────────────────────────┐
│ 前端層 (React + TypeScript + Material-UI)                  │
├─────────────────────────────────────────────────────────────┤
│ API 層 (FastAPI + Pydantic + Swagger)                     │
├─────────────────────────────────────────────────────────────┤
│ 業務邏輯層 (Python + Celery + Redis)                      │
├─────────────────────────────────────────────────────────────┤
│ ETL 層 (aiohttp + BeautifulSoup + AsyncIO)               │
├─────────────────────────────────────────────────────────────┤
│ 數據存儲層 (Redis + PostgreSQL + MongoDB)                 │
└─────────────────────────────────────────────────────────────┘
```

## 2. 程式語言與運行環境

### 2.1 Python 3.8+

**選擇理由:**

- 豐富的網路爬蟲生態系統
- 優秀的異步編程支持 (asyncio)
- 強大的第三方庫支持
- 開發效率高，代碼可讀性強

**版本要求:**

- Python 3.8+ (支援 asyncio 和 type hints)
- 建議使用 Python 3.10+ 獲得更好的性能

## 3. 異步 HTTP 客戶端

### 3.1 aiohttp

**選擇理由:**

- 高性能異步 HTTP 客戶端/服務器
- 支援 HTTP/HTTPS 代理
- 內建連接池管理
- 支援會話管理和 Cookie 處理
- 與 asyncio 完美整合

**替代方案:**

- httpx: 更現代的 HTTP 客戶端，但生態系統較新
- requests + asyncio: 同步庫的異步包裝，性能較差

**使用場景:**

```python
import aiohttp

async def fetch_with_proxy(session, url, proxy):
    async with session.get(url, proxy=proxy) as response:
        return await response.text()
```

## 4. HTML/XML 解析

### 4.1 BeautifulSoup4 + lxml

**選擇理由:**

- 最流行的 HTML 解析庫
- 支援多種解析器 (lxml, html.parser)
- 簡單易用的 API
- 強大的 CSS 選擇器支持
- 良好的錯誤處理

**替代方案:**

- Parsel: 基於 lxml，性能更好但 API 較複雜
- Scrapy Selectors: 功能強大但過於重量級

**使用場景:**

```python
from bs4 import BeautifulSoup

def parse_html(html_content):
    soup = BeautifulSoup(html_content, 'lxml')
    proxies = []
    for row in soup.select('table tr'):
        # 解析代理IP信息
        pass
    return proxies
```

### 4.2 特殊場景: Playwright

**選擇理由:**

- 處理 JavaScript 動態內容
- 模擬真實瀏覽器行為
- 支援複雜的用戶交互
- 適用於反反爬蟲場景

**使用場景:**

```python
from playwright.async_api import async_playwright

async def fetch_spys_one():
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()
        await page.goto('https://spys.one/en/http-proxy-list/')
        # 點擊下拉選單選擇500筆
        await page.select_option('select[name="xpp"]', '5')
        content = await page.content()
        await browser.close()
        return content
```

## 5. 資料庫與快取

### 5.1 Redis

**選擇理由:**

- 高性能記憶體資料庫
- 豐富的資料結構 (Sorted Set, Hash, List)
- 支援持久化
- 優秀的併發性能
- 廣泛的 Python 客戶端支持

**資料結構設計:**

```python
# 使用 Sorted Set 存儲代理IP，以響應時間為分數
ZADD proxy_pool {response_time} {ip:port:protocol}

# 使用 Hash 存儲詳細信息
HSET proxy:{ip}:{port} protocol http anonymity high source 89ip.cn
```

**替代方案:**

- MongoDB: 文檔資料庫，但性能不如 Redis
- PostgreSQL: 關係型資料庫，過於重量級
- SQLite: 輕量級，但不適合高併發

### 5.2 redis-py

**選擇理由:**

- 官方推薦的 Python Redis 客戶端
- 支援異步操作 (aioredis)
- 功能完整，性能優秀
- 良好的錯誤處理

## 6. 任務調度

### 6.1 APScheduler (Advanced Python Scheduler)

**選擇理由:**

- 功能強大的 Python 任務調度庫
- 支援多種觸發器 (cron, interval, date)
- 支援持久化存儲
- 支援分散式調度
- 與 asyncio 兼容

**替代方案:**

- Celery: 功能更強大但配置複雜
- schedule: 簡單但功能有限
- cron: 系統級調度，不夠靈活

**使用場景:**

```python
from apscheduler.schedulers.asyncio import AsyncIOScheduler

scheduler = AsyncIOScheduler()

# 每30分鐘爬取
scheduler.add_job(fetch_proxies, 'interval', minutes=30)

# 每10分鐘驗證
scheduler.add_job(validate_proxies, 'interval', minutes=10)
```

## 7. 配置管理

### 7.1 PyYAML

**選擇理由:**

- 人類可讀的配置格式
- 支援複雜的數據結構
- 廣泛使用，生態系統完善
- 與 Python 數據類型無縫轉換

**替代方案:**

- JSON: 簡單但可讀性較差
- TOML: 現代格式但生態系統較小
- INI: 功能有限

### 7.2 pydantic

**選擇理由:**

- 數據驗證和序列化
- 類型提示支持
- 自動生成文檔
- 與 FastAPI 完美整合

**使用場景:**

```python
from pydantic import BaseModel

class ProxyConfig(BaseModel):
    name: str
    url_template: str
    pages: List[int]
    interval: int
    type: str
```

## 8. 日誌與監控

### 8.1 Python logging

**選擇理由:**

- Python 標準庫，無需額外依賴
- 功能完整，支援多種處理器
- 支援異步日誌
- 可配置性強

### 8.2 structlog

**選擇理由:**

- 結構化日誌
- 更好的可讀性和可搜索性
- 支援 JSON 格式輸出
- 與現有 logging 模組兼容

**使用場景:**

```python
import structlog

logger = structlog.get_logger()

logger.info("proxy_validated",
           ip="192.168.1.1",
           port=8080,
           response_time=1.23)
```

## 9. 測試框架

### 9.1 pytest

**選擇理由:**

- 簡潔的語法
- 強大的 fixture 系統
- 支援異步測試
- 豐富的插件生態

### 9.2 pytest-asyncio

**選擇理由:**

- 專門用於異步測試
- 與 pytest 完美整合
- 支援異步 fixture

**使用場景:**

```python
import pytest

@pytest.mark.asyncio
async def test_proxy_validation():
    validator = ProxyValidator()
    result = await validator.validate_proxy(proxy)
    assert result.is_valid
```

## 10. 開發工具

### 10.1 代碼質量

- **black**: 代碼格式化
- **flake8**: 代碼風格檢查
- **mypy**: 類型檢查
- **pre-commit**: Git hooks

### 10.2 依賴管理

- **pip-tools**: 依賴版本鎖定
- **poetry**: 現代 Python 依賴管理

### 10.3 容器化

- **Docker**: 應用容器化
- **docker-compose**: 多容器編排

## 11. 部署與運維

### 11.1 進程管理

- **supervisor**: 進程監控和管理
- **systemd**: 系統級服務管理

### 11.2 監控

- **prometheus**: 指標收集
- **grafana**: 數據可視化
- **sentry**: 錯誤追蹤

## 12. 完整技術棧清單

### 12.1 核心依賴

```
aiohttp>=3.8.0
beautifulsoup4>=4.11.0
lxml>=4.9.0
redis>=4.5.0
aioredis>=2.0.0
apscheduler>=3.10.0
playwright>=1.30.0
pydantic>=1.10.0
PyYAML>=6.0
structlog>=22.3.0
```

### 12.2 開發依賴

```
pytest>=7.2.0
pytest-asyncio>=0.21.0
black>=22.12.0
flake8>=6.0.0
mypy>=0.991
pre-commit>=2.21.0
```

### 12.3 可選依賴

```
httpx>=0.24.0  # 備用HTTP客戶端
celery>=5.2.0  # 分散式任務隊列
fastapi>=0.85.0  # API服務
uvicorn>=0.20.0  # ASGI服務器
```

## 13. 性能優化建議

### 13.1 異步優化

- 使用連接池限制併發數
- 實現請求重試機制
- 使用異步上下文管理器

### 13.2 記憶體優化

- 使用生成器處理大量數據
- 及時釋放不需要的對象
- 監控記憶體使用情況

### 13.3 網路優化

- 實現請求去重
- 使用適當的 User-Agent 輪換
- 實現請求間隔控制

## 14. 安全性考慮

### 14.1 請求安全

- 實現請求頻率限制
- 使用隨機 User-Agent
- 實現 IP 輪換機制

### 14.2 數據安全

- 敏感配置加密存儲
- 實現訪問日誌記錄
- 定期清理過期數據

## 15. 擴展性設計

### 15.1 模組化架構

- 每個代理來源獨立模組
- 可插拔的驗證策略
- 可配置的儲存後端

### 15.2 水平擴展

- 支援多實例部署
- 實現負載均衡
- 支援分散式調度

## 16. 實施建議與最佳實踐

### 16.1 開發環境建議

**虛擬環境管理：**
```bash
# 使用 uv 進行高效的依賴管理
uv venv proxy_collector_env
uv shell
uv pip install -r requirements.txt
```

**項目結構建議：**
```
proxy_ip_collector/
├── src/
│   ├── collectors/          # 爬取模組
│   │   ├── __init__.py
│   │   ├── base_collector.py
│   │   ├── html_collector.py
│   │   ├── api_collector.py
│   │   └── playwright_collector.py
│   ├── validators/          # 驗證模組
│   │   ├── __init__.py
│   │   ├── proxy_validator.py
│   │   └── speed_tester.py
│   ├── storage/             # 儲存模組
│   │   ├── __init__.py
│   │   ├── redis_client.py
│   │   └── backup_manager.py
│   ├── scheduler/           # 調度模組
│   │   ├── __init__.py
│   │   └── task_scheduler.py
│   ├── models/              # 資料模型
│   │   ├── __init__.py
│   │   └── proxy_models.py
│   ├── utils/               # 工具函數
│   │   ├── __init__.py
│   │   ├── logger.py
│   │   └── config_loader.py
│   └── main.py              # 主程序入口
├── config/
│   ├── config.yaml          # 主配置文件
│   ├── sites_config.yaml    # 網站配置
│   └── logging_config.yaml  # 日誌配置
├── tests/                   # 測試文件
├── docs/                    # 文檔
├── requirements.txt         # 依賴列表
├── pyproject.toml          # 項目元數據
└── README.md               # 項目說明
```

### 16.2 配置管理最佳實踐

**環境變數管理：**
```python
from pydantic import BaseSettings

class Settings(BaseSettings):
    redis_host: str = "localhost"
    redis_port: int = 6379
    redis_password: str = ""
    max_concurrent_requests: int = 100
    request_timeout: int = 10
    
    class Config:
        env_file = ".env"
        env_prefix = "PROXY_"

settings = Settings()
```

**網站配置模板：**
```yaml
# sites_config.yaml
sites:
  - name: "89ip"
    type: "html"
    base_url: "https://www.89ip.cn/index_{page}.html"
    pages: [1, 100]
    interval: 1800  # 30分鐘
    selectors:
      ip: "table tr td:nth-child(1)"
      port: "table tr td:nth-child(2)"
      protocol: "table tr td:nth-child(3)"
    
  - name: "geonode"
    type: "api"
    base_url: "https://proxylist.geonode.com/api/proxy-list"
    params:
      limit: 500
      page: "{page}"
      sort_by: "lastChecked"
      sort_type: "desc"
    pages: [1, 24]
    interval: 1800
```

### 16.3 錯誤處理策略

**重試機制：**
```python
import asyncio
from tenacity import retry, stop_after_attempt, wait_exponential

class BaseCollector:
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def fetch_with_retry(self, session, url, proxy=None):
        try:
            async with session.get(url, proxy=proxy, timeout=30) as response:
                return await response.text()
        except asyncio.TimeoutError:
            logger.warning(f"Timeout fetching {url}")
            raise
        except Exception as e:
            logger.error(f"Error fetching {url}: {e}")
            raise
```

**降級策略：**
```python
class ProxyValidator:
    async def validate_proxy(self, proxy, test_urls=None):
        if test_urls is None:
            test_urls = [
                "http://httpbin.org/ip",
                "https://www.baidu.com",
                "https://www.google.com"
            ]
        
        for url in test_urls:
            try:
                result = await self._test_proxy(proxy, url)
                if result.is_valid:
                    return result
            except Exception as e:
                logger.debug(f"Failed to test {proxy} with {url}: {e}")
                continue
        
        return ValidationResult(is_valid=False, error="All test URLs failed")
```

### 16.4 性能優化建議

**連接池配置：**
```python
import aiohttp
import ssl

class ConnectionPool:
    def __init__(self):
        self.connector = aiohttp.TCPConnector(
            limit=100,                    # 總連接數限制
            limit_per_host=10,           # 每主機連接數限制
            ttl_dns_cache=300,          # DNS快取時間
            use_dns_cache=True,         # 啟用DNS快取
            ssl=ssl.create_default_context(),
            keepalive_timeout=30        # 保持連接超時
        )
        
        self.timeout = aiohttp.ClientTimeout(
            total=30,                   # 總超時
            connect=10,                 # 連接超時
            sock_read=20                # 讀取超時
        )
    
    def create_session(self):
        return aiohttp.ClientSession(
            connector=self.connector,
            timeout=self.timeout,
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
        )
```

**異步併發控制：**
```python
import asyncio
from asyncio import Semaphore

class ConcurrentValidator:
    def __init__(self, max_concurrent=50):
        self.semaphore = Semaphore(max_concurrent)
        self.results = []
    
    async def validate_batch(self, proxies):
        tasks = []
        for proxy in proxies:
            task = asyncio.create_task(self._validate_with_semaphore(proxy))
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return [r for r in results if not isinstance(r, Exception)]
    
    async def _validate_with_semaphore(self, proxy):
        async with self.semaphore:
            return await self._validate_proxy(proxy)
```

### 16.5 監控與告警

**健康檢查：**
```python
import asyncio
import time
from dataclasses import dataclass

@dataclass
class HealthStatus:
    redis_connected: bool
    pool_size: int
    last_update: float
    error_rate: float
    avg_response_time: float

class HealthChecker:
    async def check_health(self) -> HealthStatus:
        redis_status = await self._check_redis()
        pool_stats = await self._get_pool_stats()
        
        return HealthStatus(
            redis_connected=redis_status,
            pool_size=pool_stats['size'],
            last_update=pool_stats['last_update'],
            error_rate=pool_stats['error_rate'],
            avg_response_time=pool_stats['avg_response_time']
        )
```

**日誌監控：**
```python
import structlog
from datetime import datetime

logger = structlog.get_logger()

class MetricsCollector:
    def __init__(self):
        self.metrics = {
            'proxies_collected': 0,
            'proxies_validated': 0,
            'proxies_failed': 0,
            'validation_errors': 0
        }
    
    def log_metrics(self):
        logger.info(
            "metrics_collected",
            timestamp=datetime.now().isoformat(),
            **self.metrics
        )
```

### 16.6 部署建議

**Docker配置：**
```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

# 安裝系統依賴
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# 複製依賴文件
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# 安裝Playwright瀏覽器
RUN playwright install chromium

# 複製應用代碼
COPY . .

# 創建非root用戶
RUN useradd -m -u 1000 collector && chown -R collector:collector /app
USER collector

# 運行應用
CMD ["python", "-m", "src.main"]
```

**Docker Compose配置：**
```yaml
# docker-compose.yml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes

  proxy_collector:
    build: .
    depends_on:
      - redis
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - LOG_LEVEL=INFO
    volumes:
      - ./config:/app/config
      - ./logs:/app/logs
    restart: unless-stopped

volumes:
  redis_data:

這個技術棧選擇基於項目的具體需求，平衡了性能、穩定性和開發效率，為 Proxy IP Pool Collector 提供了堅實的技術基礎。

## 19. 新架構技術整合

### 19.1 ETL 架構技術棧

根據《ETL架構規格書.md》，ETL層技術選型：

#### 19.1.1 數據提取層
- **核心庫**: aiohttp + asyncio
- **HTML解析**: BeautifulSoup4 + lxml
- **JSON處理**: Python內置json庫
- **異步調度**: asyncio.Semaphore (並發控制)

#### 19.1.2 數據轉換層
- **數據驗證**: pydantic (類型安全)
- **數據清洗**: pandas (批量處理)
- **品質評分**: 自定義算法 + numpy
- **格式標準化**: Python dataclasses

#### 19.1.3 數據載入層
- **Redis操作**: redis-py + aioredis
- **PostgreSQL**: asyncpg (異步驅動)
- **批量載入**: COPY命令優化
- **事務處理**: asyncpg.transaction

### 19.2 後端架構技術棧

根據《後端架構規格書.md》，後端技術選型：

#### 19.2.1 Web框架
- **FastAPI**: 現代異步Web框架
- **Pydantic**: 數據驗證與序列化
- **Starlette**: ASGI工具集
- **Uvicorn**: ASGI服務器

#### 19.2.2 數據存儲
- **PostgreSQL**: 主要關係型數據庫
- **Redis**: 快取與會話存儲
- **SQLAlchemy**: ORM框架 (異步支持)
- **Alembic**: 數據庫遷移工具

#### 19.2.3 任務調度
- **Celery**: 分散式任務隊列
- **Redis Broker**: 消息中介層
- **Flower**: 任務監控工具
- **RabbitMQ**: 可替代的消息隊列

#### 19.2.4 API設計
- **RESTful API**: 資源導向設計
- **GraphQL**: 可選的查詢語言
- **WebSocket**: 實時通信支持
- **JWT認證**: 無狀態身份驗證

### 19.3 前端架構技術棧

根據《前端架構規格書.md》，前端技術選型：

#### 19.3.1 核心框架
- **React 18**: 現代UI框架
- **TypeScript**: 類型安全的JavaScript
- **Redux Toolkit**: 狀態管理
- **React Router v6**: 路由管理

#### 19.3.2 UI組件庫
- **Material-UI (MUI)**: Google設計語言
- **Tailwind CSS**: 實用優先的CSS框架
- **Styled Components**: CSS-in-JS方案
- **Framer Motion**: 動畫庫

#### 19.3.3 數據可視化
- **Chart.js**: 簡單易用的圖表庫
- **D3.js**: 可定制的數據可視化
- **Recharts**: React專用圖表庫
- **ApexCharts**: 現代圖表庫

#### 19.3.4 開發工具
- **Vite**: 快速構建工具
- **ESLint**: 代碼質量檢查
- **Prettier**: 代碼格式化
- **Jest**: 測試框架

### 19.4 架構整合優勢

#### 19.4.1 技術協同效應
1. **前後端分離**: React + FastAPI 完美配合
2. **異步處理**: 從ETL到API全鏈路異步
3. **類型安全**: TypeScript + Pydantic 雙重保障
4. **實時通信**: WebSocket 支持實時監控

#### 19.4.2 開發效率提升
1. **熱重載**: 前後端均支援熱重載
2. **自動文檔**: OpenAPI 自動生成
3. **組件復用**: 模組化設計
4. **統一配置**: 集中化配置管理

#### 19.4.3 運維便利性
1. **容器化**: Docker 支援所有組件
2. **監控整合**: 統一的監控指標
3. **日誌集中**: 結構化日誌輸出
4. **健康檢查**: 全面的健康檢查端點

## 17. 進階優化策略

### 17.1 智能爬取優化

#### 17.1.1 自適應請求頻率
```python
from collections import defaultdict
import time

class AdaptiveRateLimiter:
    """自適應請求頻率限制器"""
    
    def __init__(self, base_delay=1.0, max_delay=60.0, min_delay=0.1):
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.min_delay = min_delay
        self.domain_delays = defaultdict(lambda: base_delay)
        self.domain_stats = defaultdict(lambda: {'success': 0, 'failure': 0})
    
    async def get_delay(self, domain: str) -> float:
        """獲取指定域名的請求延遲"""
        return self.domain_delays[domain]
    
    async def update_delay(self, domain: str, success: bool):
        """根據請求結果更新延遲"""
        stats = self.domain_stats[domain]
        if success:
            stats['success'] += 1
            # 成功時減少延遲
            self.domain_delays[domain] = max(
                self.min_delay,
                self.domain_delays[domain] * 0.8
            )
        else:
            stats['failure'] += 1
            # 失敗時增加延遲
            self.domain_delays[domain] = min(
                self.max_delay,
                self.domain_delays[domain] * 1.5
            )
```

#### 17.1.2 網站健康度監控
```python
class WebsiteHealthMonitor:
    """網站健康度監控器"""
    
    def __init__(self):
        self.health_stats = defaultdict(lambda: {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'avg_response_time': 0,
            'last_check': None,
            'status': 'unknown'
        })
    
    def record_request(self, domain: str, success: bool, response_time: float):
        """記錄請求結果"""
        stats = self.health_stats[domain]
        stats['total_requests'] += 1
        stats['last_check'] = time.time()
        
        if success:
            stats['successful_requests'] += 1
            # 更新平均響應時間
            if stats['avg_response_time'] == 0:
                stats['avg_response_time'] = response_time
            else:
                stats['avg_response_time'] = (
                    stats['avg_response_time'] * 0.8 + response_time * 0.2
                )
        else:
            stats['failed_requests'] += 1
        
        # 更新狀態
        success_rate = stats['successful_requests'] / stats['total_requests']
        if success_rate >= 0.9:
            stats['status'] = 'healthy'
        elif success_rate >= 0.7:
            stats['status'] = 'degraded'
        else:
            stats['status'] = 'unhealthy'
    
    def get_recommendations(self) -> List[str]:
        """獲取網站優化建議"""
        recommendations = []
        for domain, stats in self.health_stats.items():
            if stats['status'] == 'unhealthy':
                recommendations.append(f"暫停 {domain} 的爬取，等待恢復")
            elif stats['status'] == 'degraded':
                recommendations.append(f"降低 {domain} 的爬取頻率")
            elif stats['avg_response_time'] > 5:
                recommendations.append(f"{domain} 響應較慢，考慮超時調整")
        return recommendations
```

### 17.2 代理質量評分系統

#### 17.2.1 多維度評分算法
```python
from dataclasses import dataclass
from typing import Dict, List
import statistics

@dataclass
class ProxyQualityMetrics:
    """代理質量指標"""
    response_time: float  # 響應時間（秒）
    success_rate: float   # 成功率（0-1）
    stability_score: float  # 穩定性評分（0-1）
    anonymity_level: int   # 匿名等級（1-3）
    location_score: float  # 地理位置評分（0-1）
    usage_count: int      # 使用次數
    fail_count: int       # 失敗次數

class ProxyQualityScorer:
    """代理質量評分器"""
    
    def __init__(self):
        self.weights = {
            'response_time': 0.25,
            'success_rate': 0.30,
            'stability_score': 0.20,
            'anonymity_level': 0.15,
            'location_score': 0.10
        }
    
    def calculate_score(self, metrics: ProxyQualityMetrics) -> float:
        """計算代理綜合評分"""
        # 響應時間評分（越快越好）
        response_score = max(0, 1 - metrics.response_time / 10)
        
        # 成功率評分
        success_score = metrics.success_rate
        
        # 穩定性評分（基於歷史表現）
        total_uses = metrics.usage_count + metrics.fail_count
        if total_uses > 0:
            recent_success_rate = (metrics.usage_count) / total_uses
            stability_score = statistics.mean([
                metrics.stability_score,
                recent_success_rate,
                success_score
            ])
        else:
            stability_score = metrics.stability_score
        
        # 匿名等級評分
        anonymity_score = metrics.anonymity_level / 3
        
        # 綜合評分
        score = (
            self.weights['response_time'] * response_score +
            self.weights['success_rate'] * success_score +
            self.weights['stability_score'] * stability_score +
            self.weights['anonymity_level'] * anonymity_score +
            self.weights['location_score'] * metrics.location_score
        )
        
        return min(1.0, max(0.0, score))
    
    def get_quality_tier(self, score: float) -> str:
        """獲取質量等級"""
        if score >= 0.9:
            return 'premium'
        elif score >= 0.7:
            return 'high'
        elif score >= 0.5:
            return 'medium'
        else:
            return 'low'
```

#### 17.2.2 動態評分調整
```python
class DynamicScoreAdjuster:
    """動態評分調整器"""
    
    def __init__(self, learning_rate=0.1):
        self.learning_rate = learning_rate
        self.proxy_history = defaultdict(lambda: {
            'scores': [],
            'last_adjustment': time.time(),
            'trend': 'stable'
        })
    
    def adjust_score(self, proxy_id: str, base_score: float, 
                    recent_performance: Dict) -> float:
        """根據最新表現調整評分"""
        history = self.proxy_history[proxy_id]
        
        # 計算趨勢
        if len(history['scores']) >= 3:
            recent_avg = statistics.mean(history['scores'][-3:])
            older_avg = statistics.mean(history['scores'][:-3])
            
            if recent_avg > older_avg * 1.1:
                trend = 'improving'
            elif recent_avg < older_avg * 0.9:
                trend = 'declining'
            else:
                trend = 'stable'
        else:
            trend = 'stable'
        
        history['trend'] = trend
        
        # 基於趨勢調整
        adjustment = 0
        if trend == 'improving':
            adjustment = self.learning_rate * 0.1  # 小幅提升
        elif trend == 'declining':
            adjustment = -self.learning_rate * 0.2  # 較大降幅
        
        # 基於最新表現調整
        if recent_performance.get('success', True):
            adjustment += self.learning_rate * 0.05
        else:
            adjustment -= self.learning_rate * 0.15
        
        new_score = base_score + adjustment
        new_score = min(1.0, max(0.0, new_score))
        
        # 更新歷史
        history['scores'].append(new_score)
        if len(history['scores']) > 100:  # 限制歷史記錄長度
            history['scores'].pop(0)
        
        return new_score
```

### 17.3 資源使用優化

#### 17.3.1 內存管理優化
```python
import weakref
import gc
from contextlib import contextmanager

class MemoryOptimizer:
    """內存使用優化器"""
    
    def __init__(self, max_memory_mb: int = 500):
        self.max_memory_mb = max_memory_mb
        self.object_pool = weakref.WeakSet()
        self.cleanup_threshold = 0.8  # 達到80%時清理
    
    @contextmanager
    def optimized_context(self):
        """內存優化上下文"""
        try:
            yield self
        finally:
            self._cleanup()
    
    def _cleanup(self):
        """清理內存"""
        # 強制垃圾回收
        gc.collect()
        
        # 清理對象池
        self.object_pool.clear()
    
    def monitor_memory(self) -> Dict[str, float]:
        """監控內存使用"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        
        return {
            'rss_mb': memory_info.rss / 1024 / 1024,  # 實際使用內存
            'vms_mb': memory_info.vms / 1024 / 1024,  # 虛擬內存
            'percent': memory_info.rss / 1024 / 1024 / self.max_memory_mb
        }
    
    def should_cleanup(self) -> bool:
        """判斷是否需要清理"""
        memory_info = self.monitor_memory()
        return memory_info['percent'] > self.cleanup_threshold
```

#### 17.3.2 連接池智能管理
```python
class SmartConnectionPool:
    """智能連接池管理"""
    
    def __init__(self, min_connections: int = 5, max_connections: int = 20):
        self.min_connections = min_connections
        self.max_connections = max_connections
        self.current_connections = min_connections
        self.connection_stats = defaultdict(lambda: {
            'usage_count': 0,
            'last_used': 0,
            'errors': 0
        })
    
    async def adjust_pool_size(self, load_factor: float):
        """根據負載因子調整池大小"""
        if load_factor > 0.8 and self.current_connections < self.max_connections:
            # 高負載時增加連接
            self.current_connections = min(
                self.current_connections + 2, 
                self.max_connections
            )
        elif load_factor < 0.3 and self.current_connections > self.min_connections:
            # 低負載時減少連接
            self.current_connections = max(
                self.current_connections - 1,
                self.min_connections
            )
    
    def get_load_factor(self) -> float:
        """獲取當前負載因子"""
        active_connections = sum(
            1 for stats in self.connection_stats.values()
            if time.time() - stats['last_used'] < 60  # 最近1分鐘內使用
        )
        return active_connections / self.current_connections
```

### 17.4 高級錯誤處理

#### 17.4.1 智能重試策略
```python
import random
import math
from enum import Enum

class RetryStrategy(Enum):
    FIXED = "fixed"
    LINEAR = "linear"
    EXPONENTIAL = "exponential"
    CUSTOM = "custom"

class SmartRetryHandler:
    """智能重試處理器"""
    
    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.retry_stats = defaultdict(lambda: {
            'attempts': 0,
            'successes': 0,
            'failures': 0
        })
    
    async def execute_with_retry(self, func, *args, 
                               strategy: RetryStrategy = RetryStrategy.EXPONENTIAL,
                               **kwargs):
        """執行帶重試的函數"""
        last_exception = None
        
        for attempt in range(self.max_retries + 1):
            try:
                result = await func(*args, **kwargs)
                
                # 記錄成功
                func_name = func.__name__
                self.retry_stats[func_name]['attempts'] += 1
                self.retry_stats[func_name]['successes'] += 1
                
                return result
                
            except Exception as e:
                last_exception = e
                func_name = func.__name__
                self.retry_stats[func_name]['attempts'] += 1
                self.retry_stats[func_name]['failures'] += 1
                
                if attempt == self.max_retries:
                    break
                
                # 計算延遲
                delay = self._calculate_delay(attempt, strategy)
                await asyncio.sleep(delay)
        
        # 所有重試都失敗
        raise last_exception
    
    def _calculate_delay(self, attempt: int, strategy: RetryStrategy) -> float:
        """計算重試延遲"""
        if strategy == RetryStrategy.FIXED:
            return self.base_delay
        elif strategy == RetryStrategy.LINEAR:
            return self.base_delay * (attempt + 1)
        elif strategy == RetryStrategy.EXPONENTIAL:
            return self.base_delay * (2 ** attempt) + random.uniform(0, 1)
        else:
            # 自定義策略：根據歷史統計調整
            return self._custom_delay(attempt)
    
    def _custom_delay(self, attempt: int) -> float:
        """自定義延遲策略"""
        # 基於歷史成功率調整
        base_delay = self.base_delay
        
        # 添加一些隨機性避免雷擊
        jitter = random.uniform(0.5, 1.5)
        
        return base_delay * math.sqrt(attempt + 1) * jitter
```

#### 17.4.2 錯誤分類與處理
```python
class ErrorClassifier:
    """錯誤分類器"""
    
    def __init__(self):
        self.error_patterns = {
            'network': [
                'ConnectionError', 'TimeoutError', 'NetworkError',
                'DNS resolution failed', 'Connection refused'
            ],
            'proxy': [
                'ProxyError', 'SSLError', 'Proxy connection failed',
                'Bad proxy', 'Proxy timeout'
            ],
            'rate_limit': [
                'Too Many Requests', 'Rate limit exceeded', '429',
                'Request throttled', 'Quota exceeded'
            ],
            'content': [
                'ParseError', 'Invalid content', 'JSON decode error',
                'HTML parsing failed'
            ],
            'server': [
                'Internal Server Error', '502', '503', '504',
                'Service temporarily unavailable'
            ]
        }
    
    def classify_error(self, error: Exception) -> str:
        """分類錯誤類型"""
        error_message = str(error).lower()
        
        for category, patterns in self.error_patterns.items():
            for pattern in patterns:
                if pattern.lower() in error_message:
                    return category
        
        return 'unknown'
    
    def get_recommendation(self, error_category: str, 
                          context: Dict = None) -> str:
        """獲取錯誤處理建議"""
        recommendations = {
            'network': '檢查網絡連接，增加超時時間，使用備用DNS',
            'proxy': '更換代理IP，檢查代理設置，降低請求頻率',
            'rate_limit': '降低請求頻率，使用代理輪換，添加延遲',
            'content': '檢查目標網站結構，更新解析規則，驗證數據格式',
            'server': '等待服務恢復，使用備用數據源，實施退避策略',
            'unknown': '記錄詳細錯誤信息，進行人工分析，考慮跳過該任務'
        }
        
        return recommendations.get(error_category, 
                               recommendations['unknown'])
